# my_attention
This repo aims to practice the understanding and the implementation of the attention mechanism (self-attention &amp; multi-head attention) from the Transformers arch
